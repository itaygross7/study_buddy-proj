"""
AI Middleware - Lightweight Microservice Layer

MICROSERVICE PRINCIPLE: Keep it small, focused, and async
- This service ONLY handles prompt optimization and response adaptation
- All processing flows through RabbitMQ queues
- No blocking operations
- Fast forwarding to workers

ðŸ”’ CRITICAL: Enforces document-only constraints to prevent hallucinations

ARCHITECTURE:
User Request â†’ Queue â†’ Prompt Optimizer â†’ Main AI Queue â†’ Output Adapter â†’ User

Cost: ~2 small gpt-4o-mini calls (~300-500 tokens each) = $0.0002
Benefit: 10x better UX for non-tech users + Guaranteed accuracy
"""

from typing import Optional, Dict, Any
from dataclasses import dataclass, asdict
from pymongo.database import Database
import openai
import json

from src.infrastructure.config import settings
from src.infrastructure.database import db as flask_db
from src.utils.ai_constraints import (
    build_constrained_context, 
    get_task_constraint_level,
    validate_response_constraint
)
from sb_utils.logger_utils import logger


@dataclass
class UserPreferences:
    """Lightweight user preferences - stored in DB, cached in memory."""
    user_id: str
    language: str = "he"
    proficiency_level: str = "intermediate"  # beginner, intermediate, advanced
    explanation_style: str = "detailed"  # concise, detailed, step_by_step
    use_examples: bool = True
    use_analogies: bool = True
    baby_mode: bool = False
    preferred_formats: list = None
    
    def __post_init__(self):
        if self.preferred_formats is None:
            self.preferred_formats = ['bullet_points']
    
    def to_dict(self):
        """Convert to dict for JSON serialization."""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: dict):
        """Create from dict."""
        return cls(**data)


class PromptOptimizer:
    """
    Microservice: Prompt Optimization
    Input: Raw user request â†’ Output: Optimized prompt
    Fast, lightweight, single responsibility
    
    ðŸ”’ ENFORCES app requirements (document-only constraint)
    ðŸ‘¤ RESPECTS user preferences
    """
    
    @staticmethod
    def optimize(
        user_request: str, 
        task_type: str, 
        user_prefs: UserPreferences,
        document_content: str = ""
    ) -> Dict[str, str]:
        """
        FAST optimization: 200-300 tokens, <0.5s response time
        
        ðŸ”’ CRITICAL: Injects app requirements into optimization
        - Document-only constraint (MANDATORY)
        - User preferences (OPTIONAL)
        
        Args:
            user_request: Raw user input
            task_type: Type of task
            user_prefs: User preferences
            document_content: The document content (to determine if constraint needed)
            
        Returns:
            Dict with optimized_prompt and system_context
        """
        if not settings.OPENAI_API_KEY:
            return {'optimized_prompt': user_request, 'system_context': ''}
        
        # Determine constraint level
        constraint_level = get_task_constraint_level(task_type)
        has_document = bool(document_content and len(document_content.strip()) > 0)
        
        # Build APP REQUIREMENTS (mandatory rules)
        app_requirements = []
        
        if constraint_level == "strict" and has_document:
            app_requirements.append("ðŸ”’ MANDATORY: Answer ONLY from provided document. NO external knowledge.")
        elif constraint_level == "moderate" and has_document:
            app_requirements.append("ðŸ“š MANDATORY: Prioritize document content. Indicate when using external knowledge.")
        
        # Build meta-prompt with APP REQUIREMENTS + USER PREFERENCES
        requirements_text = "\n".join(app_requirements) if app_requirements else ""
        
        meta_prompt = f"""You are optimizing a prompt for educational AI.

USER REQUEST: "{user_request}"

APP REQUIREMENTS (MUST include):
{requirements_text if requirements_text else "No special requirements"}

USER PREFERENCES (should include):
- Task: {task_type}
- Level: {user_prefs.proficiency_level}
- Language: {user_prefs.language}
- Style: {user_prefs.explanation_style}
{"- Include examples/analogies" if user_prefs.use_examples else ""}

OPTIMIZE to:
1. Make request clear and specific
2. ENFORCE app requirements (document-only if applicable)
3. Match user preferences
4. Request appropriate format

JSON output:
{{"optimized_prompt": "...", "system_context": "..."}}"""

        try:
            client = openai.OpenAI(api_key=settings.OPENAI_API_KEY, timeout=5.0)
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": meta_prompt}],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_tokens=300  # SMALL: keep it fast
            )
            
            result = json.loads(response.choices[0].message.content)
            logger.debug(f"âœ“ Prompt optimized with {constraint_level} constraint")
            
            return {
                'optimized_prompt': result.get('optimized_prompt', user_request),
                'system_context': result.get('system_context', '')
            }
            
        except Exception as e:
            logger.warning(f"Prompt optimization failed: {e}")
            return {'optimized_prompt': user_request, 'system_context': ''}


class ResponseAdapter:
    """
    Microservice: Response Adaptation
    Input: Raw AI response â†’ Output: User-optimized response
    Fast, lightweight, single responsibility
    """
    
    @staticmethod
    def adapt(ai_response: str, user_prefs: UserPreferences, task_type: str) -> str:
        """
        FAST adaptation: Smart token sizing, <0.8s response time
        
        Args:
            ai_response: Raw AI output
            user_prefs: User preferences
            task_type: Type of task
            
        Returns:
            Adapted response
        """
        if not settings.OPENAI_API_KEY:
            return ai_response
        
        # Skip if already optimized or too short
        if len(ai_response) < 100 or "ðŸ¼" in ai_response:
            return ai_response
        
        # Ultra-concise adaptation prompt
        adaptation_prompt = f"""Adapt for {user_prefs.proficiency_level}:
{ai_response}

Style: {user_prefs.explanation_style}
{"Add examples" if user_prefs.use_examples else ""}

Output only adapted text."""

        try:
            client = openai.OpenAI(api_key=settings.OPENAI_API_KEY, timeout=8.0)
            
            # Smart sizing: don't waste tokens
            input_tokens = len(ai_response.split())
            max_tokens = min(800, input_tokens + 200)
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": adaptation_prompt}],
                temperature=0.3,
                max_tokens=max_tokens
            )
            
            adapted = response.choices[0].message.content.strip()
            logger.debug(f"âœ“ Response adapted (fast)")
            return adapted
            
        except Exception as e:
            logger.warning(f"Response adaptation failed: {e}")
            return ai_response


class PreferencesService:
    """
    Microservice: User Preferences Management
    Single responsibility: Load/save user preferences
    """
    
    def __init__(self, db_conn: Database = None):
        self.db = db_conn or flask_db
        self._cache = {}  # Simple cache for performance
    
    def get(self, user_id: str, use_cache: bool = True) -> UserPreferences:
        """Load preferences (cached)."""
        if use_cache and user_id in self._cache:
            return self._cache[user_id]
        
        try:
            doc = self.db.user_preferences.find_one({"user_id": user_id})
            prefs = UserPreferences.from_dict(doc) if doc else UserPreferences(user_id=user_id)
            self._cache[user_id] = prefs
            return prefs
        except Exception as e:
            logger.error(f"Failed to load preferences: {e}")
            return UserPreferences(user_id=user_id)
    
    def save(self, prefs: UserPreferences):
        """Save preferences and update cache."""
        try:
            self.db.user_preferences.update_one(
                {"user_id": prefs.user_id},
                {"$set": prefs.to_dict()},
                upsert=True
            )
            self._cache[prefs.user_id] = prefs
            logger.debug(f"âœ“ Saved preferences for {prefs.user_id}")
        except Exception as e:
            logger.error(f"Failed to save preferences: {e}")
    
    def clear_cache(self, user_id: Optional[str] = None):
        """Clear cache."""
        if user_id:
            self._cache.pop(user_id, None)
        else:
            self._cache.clear()


class AIMiddleware:
    """
    LIGHTWEIGHT Microservice Orchestrator
    
    DESIGN PRINCIPLES:
    - Single Responsibility: Coordinate prompt optimization + response adaptation
    - Stateless: No heavy processing, just orchestration
    - Fast: All operations are async-ready
    - Microservice: Small, focused, one job
    
    FLOW:
    1. User request â†’ optimize_prompt() â†’ queue message
    2. Queue worker â†’ AI processing â†’ result
    3. Result â†’ adapt_response() â†’ user
    """
    
    def __init__(self, db_conn: Database = None):
        self.prefs_service = PreferencesService(db_conn)
        self.prompt_optimizer = PromptOptimizer()
        self.response_adapter = ResponseAdapter()
    
    def prepare_request(
        self,
        user_request: str,
        document_content: str,
        task_type: str,
        user_id: str
    ) -> Dict[str, Any]:
        """
        STEP 1: Prepare request for queue (microservice pattern)
        
        ðŸ”’ ALWAYS ADDS STRICT CONSTRAINTS - NO BYPASS ALLOWED
        
        This is FAST - just optimization + constraint injection
        Returns data structure ready for RabbitMQ
        
        Args:
            user_request: Raw user input
            document_content: User's document content (THE ONLY SOURCE)
            task_type: Task type
            user_id: User ID
            
        Returns:
            Dict ready for queue message with constraints ALWAYS enforced
        """
        user_prefs = self.prefs_service.get(user_id)
        
        # ðŸ”’ CRITICAL: ALWAYS BUILD CONSTRAINED CONTEXT - NO EXCEPTIONS
        constrained_context = build_constrained_context(
            task_type=task_type,
            document_content=document_content,
            user_id=user_id,  # Pass user_id for isolation tracking
            user_context="",
            language=user_prefs.language
        )
        
        constraint_level = get_task_constraint_level(task_type)
        logger.info(f"ðŸ”’ MANDATORY {constraint_level} constraint applied for {task_type}")
        
        # Optimize prompt (this ADDS to constraints, never removes them)
        optimized = self.prompt_optimizer.optimize(
            user_request=user_request,
            task_type=task_type,
            user_prefs=user_prefs,
            document_content=document_content  # Pass document for constraint detection
        )
        
        # COMBINE optimized prompt with constrained context
        # IMPORTANT: Constraints are ALWAYS included, optimization only enhances
        final_context = constrained_context + "\n\n" + optimized['system_context']
        
        return {
            'prompt': optimized['optimized_prompt'],
            'context': final_context,
            'user_prefs': user_prefs.to_dict(),
            'task_type': task_type,
            'original_request': user_request,
            'document_content': document_content,  # Keep for validation
            'constraint_level': constraint_level,
            'constraint_enforced': True  # ALWAYS True - no bypass
        }
    
    def finalize_response(
        self,
        ai_response: str,
        request_data: Dict[str, Any],
        adapt: bool = True
    ) -> str:
        """
        STEP 2: Finalize response after AI processing (microservice pattern)
        
        ðŸ”’ ALWAYS VALIDATES constraints were followed - NO BYPASS
        
        This is FAST - adaptation + mandatory validation
        
        Args:
            ai_response: Raw AI output
            request_data: Data from prepare_request
            adapt: Whether to adapt (does NOT affect constraint validation)
            
        Returns:
            Final response for user (ALWAYS validated)
        """
        # ðŸ”’ CRITICAL: ALWAYS VALIDATE CONSTRAINTS - NO EXCEPTIONS
        document_content = request_data.get('document_content', '')
        task_type = request_data.get('task_type', 'standard')
        
        validation = validate_response_constraint(
            response=ai_response,
            task_type=task_type,
            document_content=document_content
        )
        
        if not validation['valid']:
            logger.error(f"âš ï¸ CONSTRAINT VIOLATION DETECTED: {validation['warnings']}")
            # Log but continue (validation is heuristic, may have false positives)
            # In production, you might want to reject the response here
        else:
            logger.info(f"âœ“ Response passed MANDATORY {validation['constraint_level']} constraint validation")
        
        # ADAPT RESPONSE (optional, but validation above is mandatory)
        if not adapt:
            return ai_response
        
        try:
            user_prefs = UserPreferences.from_dict(request_data.get('user_prefs', {}))
            task_type = request_data.get('task_type', 'standard')
            
            adapted = self.response_adapter.adapt(
                ai_response=ai_response,
                user_prefs=user_prefs,
                task_type=task_type
            )
            return adapted
        except Exception as e:
            logger.error(f"Response adaptation failed: {e}")
            return ai_response


# Global lightweight service instances
preferences_service = PreferencesService()
ai_middleware = AIMiddleware()


def get_user_preferences(user_id: str) -> UserPreferences:
    """Helper: Get user preferences."""
    return preferences_service.get(user_id)


def save_user_preferences(prefs: UserPreferences):
    """Helper: Save user preferences."""
    preferences_service.save(prefs)


